{
  "3": {
    "inputs": {
      "seed": 1106973686485886,
      "steps": 20,
      "cfg": 5,
      "sampler_name": "dpmpp_2m_sde",
      "scheduler": "karras",
      "denoise": 0.6,
      "model": [
        "44",
        0
      ],
      "positive": [
        "60",
        0
      ],
      "negative": [
        "7",
        0
      ],
      "latent_image": [
        "54",
        0
      ]
    },
    "class_type": "KSampler"
  },
  "4": {
    "inputs": {
      "ckpt_name": "fudukiMix_v15.safetensors"
    },
    "class_type": "CheckpointLoaderSimple"
  },
  "6": {
    "inputs": {
      "text": "Bokeh, (TOK:1.5),realistic,",
      "clip": [
        "44",
        1
      ]
    },
    "class_type": "CLIPTextEncode"
  },
  "7": {
    "inputs": {
      "text": "embedding:negativeXL_D, ",
      "clip": [
        "4",
        1
      ]
    },
    "class_type": "CLIPTextEncode"
  },
  "8": {
    "inputs": {
      "samples": [
        "3",
        0
      ],
      "vae": [
        "4",
        2
      ]
    },
    "class_type": "VAEDecode"
  },
  "12": {
    "inputs": {
      "image": "00323-877177378.png",
      "choose file to upload": "image"
    },
    "class_type": "LoadImage"
  },
  "13": {
    "inputs": {
      "images": [
        "20",
        0
      ]
    },
    "class_type": "PreviewImage"
  },
  "20": {
    "inputs": {
      "facedetection": "retinaface_resnet50",
      "codeformer_fidelity": 0.6,
      "facerestore_model": [
        "21",
        0
      ],
      "image": [
        "8",
        0
      ]
    },
    "class_type": "FaceRestoreCFWithModel"
  },
  "21": {
    "inputs": {
      "model_name": "GFPGANv1.4.pth"
    },
    "class_type": "FaceRestoreModelLoader"
  },
  "44": {
    "inputs": {
      "model": [
        "4",
        0
      ],
      "clip": [
        "4",
        1
      ],
      "lora_stack": [
        "45",
        0
      ]
    },
    "class_type": "CR Apply LoRA Stack"
  },
  "45": {
    "inputs": {
      "switch_1": "On",
      "lora_name_1": "NsfwPovAllInOneLoraSdxl-000009.safetensors",
      "model_weight_1": 1,
      "clip_weight_1": 0.4,
      "switch_2": "On",
      "lora_name_2": "neg4all_bdsqlsz_xl_V7.safetensors",
      "model_weight_2": 1,
      "clip_weight_2": 0.3,
      "switch_3": "On",
      "lora_name_3": "miho_v1.safetensors",
      "model_weight_3": 1,
      "clip_weight_3": 1
    },
    "class_type": "CR LoRA Stack"
  },
  "49": {
    "inputs": {
      "pixels": [
        "50",
        0
      ],
      "vae": [
        "4",
        2
      ]
    },
    "class_type": "VAEEncode"
  },
  "50": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "megapixels": 0.8,
      "image": [
        "12",
        0
      ]
    },
    "class_type": "ImageScaleToTotalPixels"
  },
  "54": {
    "inputs": {
      "amount": 4,
      "samples": [
        "49",
        0
      ]
    },
    "class_type": "RepeatLatentBatch"
  },
  "60": {
    "inputs": {
      "strength": 0.5,
      "conditioning": [
        "6",
        0
      ],
      "control_net": [
        "61",
        0
      ],
      "image": [
        "62",
        0
      ]
    },
    "class_type": "ControlNetApply"
  },
  "61": {
    "inputs": {
      "control_net_name": "models\\t2i-adapter_diffusers_xl_canny.safetensors"
    },
    "class_type": "ControlNetLoader"
  },
  "62": {
    "inputs": {
      "preprocessor": "CannyEdgePreprocessor",
      "resolution": 512,
      "image": [
        "12",
        0
      ]
    },
    "class_type": "AIO_Preprocessor"
  }
}